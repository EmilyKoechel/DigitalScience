# -*- coding: utf-8 -*-
"""Starter Kit 01MAY2023

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12f87i6RtvA1ttlQN2UXUs_YIxrhL74YC

##Resources:
"""

##Helpful resources:
##API Data Sources and Field names: https://docs.dimensions.ai/dsl/data-sources.html 
##API and DSL Documentation: https://docs.dimensions.ai/dsl/index.html
##API Lab ("Cookbooks" for common use cases such as citation analysis): https://api-lab.dimensions.ai/ 
##   Please contact me if I can be of further help.  I'm happy to support your data pulls for a specific analysis
##   or help you with automation on commonly used datasets pulled from the API.
##   Emily Koechel, Technical PSM, Boulder, Colorado -- e.koechel@digital-science.com
##   Schedule time:  https://calendar.app.google/be2NxPDvo1EiLjCa6

"""##Python Prep and API Connection"""

!pip install requests
!pip install dimcli -U
!pip install pandasql
import requests
import datetime
import dimcli
from dimcli.utils import *
import json
import sys
import pandas as pd
import numpy as np
from pandasql import sqldf
import pandasql as ps
print("==\nCHANGELOG\nThis notebook was last run on %s\n==" % datetime.date.today().strftime('%b %d, %Y'))

dimcli.login(key="YOUR KEY HERE",
             endpoint="https://app.dimensions.ai")
dimcli
dsl = dimcli.Dsl()

"""##Simple calls with Magic and dimcli"""

#  %dsldf = single line query with up to 1000 records returned (Full count provided)
#  %%dsldf = Multiline query with up to 1000 records returned (Full count provided)
#  %dslloopdf = single line query with up to 50000 records returned (Full count provided)
#  %%dslloopdf = Multiline query with up to 1000 records returned (Full count provided)
#Magic using "%%" must be the only code in the cell with no comments preceding it

#Below are four ways of creating a Dataframe named DF_Octopus

#These will return the same results:

#Magic:
DF_Octopus = %dsldf search publications for "octopus" return publications limit 500
DF_Octopus.head(9)

#Dimcli:
DF_Octopus = dsl.query(f"""search publications for "octopus" 
                                     return publications limit 500""", verbose=True).as_dataframe()
DF_Octopus.head(100)

# Commented out IPython magic to ensure Python compatibility.
# %%dsldf DF_Octopus
# search publications
# for "Octopus"
# where research_orgs = "grid.134907.8"
# return publications[id+referenced_pubs+reference_ids] limit 100

#These will return the same results:

#Magic:
DF_Octopus = %dslloopdf search publications for "octopus" where year in [2015:2020] return publications

#Dimcli
DF_Octopus = dsl.query_iterative(f"""search publications 
                              for "octopus" where year in [2015:2020] 
                              return publications""", verbose=True).as_dataframe()
DF_Octopus.head(100)

# Commented out IPython magic to ensure Python compatibility.
# %%dslloopdf DF_Octopus
# search publications
# for "Octopus"
# where year in [2017:2020] 
# return publications[id+title+doi+authors+research_orgs]

"""##Dataframe return options with Dimcli:"""

#Each option returns different default views providing exploded details
DF_Octopus = dsl.query(f"""search publications for "octopus" 
                           return publications[id+concepts_scores+authors] 
                           limit 500""", verbose=True).as_dataframe()
DF_Octopus.head(100)

DF_Octopus = dsl.query(f"""search publications for "octopus" 
                           return publications[id+concepts_scores+authors] 
                           limit 500""", verbose=True).as_dataframe_authors()
DF_Octopus.head(100)

DF_Octopus = dsl.query(f"""search publications for "octopus" 
                           return publications[id+concepts_scores+authors] 
                           limit 500""", verbose=True).as_dataframe_authors_affiliations()
DF_Octopus.head(100)

DF_Octopus = dsl.query(f"""search publications for "octopus" 
                           return publications[id+concepts_scores+authors] 
                           limit 500""", verbose=True).as_dataframe_concepts()
DF_Octopus.head(100)

"""##ForLoop Method (Pull more than 50K records)

"""

#Each loop (Example "Animal Sciences 2022" must return fewer than 50K results
#The results of each loop are concatenated into a single dataset for further analysis/manipulation

dsl = dimcli.Dsl()
YearList=list(range(2021,2023))
KWList = (['Animal Sciences','wind energy'])
print("Keyword List: ",KWList)

BigSet=pd.DataFrame()
for k in KWList:
  for y in YearList: 
    print("****************Loop:",k,y,"*****************************************")
    PubChunk =  %dslloopdf search publications in title_abstract_only for "\"{k}\"" where year = {y} return publications[id]
    PubChunk['SearchTerm'] = k
    BigSet = pd.concat([BigSet, PubChunk])
    print("1 second pause for API Rest")
    time.sleep(1) # Seconds
    print("Running total records in BigSet:  ",len(BigSet))
BigSet.head(9)

"""##List Chunk Method (Pull more than 50K records)"""

# Using any Pandas list (which can be quite long!), search for chunks of that list iteratively
# and append results together into a single dataset
#This example will return all the information about the publications that cited the Pubs we identified in the prior call.
#Dedupe and drop nulls; create a list of the interesting publications we found above
PubList = BigSet['id'].dropna().tolist()
PubList = list(dict.fromkeys(PubList))
print(PubList)

ChunkNumber = 1
ChunkSize = 400  #<--  If you get an error, reduce this number.  Max is 500; 200 is a great starting point.
TotalChunks = round(0.5+(len(PubList)/ChunkSize))
# Find publications that cited the list above
q = """search publications where referenced_pubs in {}
              return publications[id+year+date+title+journal+abstract+category_for+doi
              +times_cited+altmetric+field_citation_ratio+relative_citation_ratio+recent_citations
              +reference_ids+linkout+open_access]"""
results = []
for chunk in (list(chunks_of(list(PubList), ChunkSize))): 
    print("Working on Chunk #",(ChunkNumber)," of ",TotalChunks)
    ChunkNumber = ChunkNumber+1
    data = dsl.query_iterative(q.format(json.dumps(chunk)), verbose=False)
    results += data.publications
    time.sleep(1)
Pubs = pd.DataFrame().from_dict(results)
print("Publications found: ", len(Pubs))
Pubs.drop_duplicates(subset='doi', inplace=True)
print("Unique publications found: ", len(Pubs))
Pubs.head(5)

"""## Pull, Explode and Flatten; Identify Author Order

Goal: Identify the publications where a researcher from My Institution or a Peer institution is a First or Last Author; create a flattened table for further analysis in Excel, PowerBI, Tableau, or the like
"""

##Run Python 'prep code' and API Connection before this

dsl = dimcli.Dsl() 
ONCOLOGY_CALL = dsl.query_iterative("""search publications
                    in title_abstract_only for "neuroscience" or for "glioma"
                    where year in [2000:2022] and research_orgs in ["grid.21925.3d","grid.147455.6","grid.25879.31","grid.29857.31"]
                    return publications[id+title+year+times_cited+authors+open_access+authors_count+journal]""", verbose=True).as_dataframe()  ##researcher_id, pub_id, current_organization_ID

#Separate Authors into separate records with duplicated pub_id information
ONCOLOGY=ONCOLOGY_CALL.explode('authors').reset_index()
Authors = pd.json_normalize(ONCOLOGY['authors'])
ONCOLOGY[['affiliations','corresponding','current_organization_id','first_name','last_name','orcid','raw_affiliation','researcher_id']] =  Authors
ONCOLOGY = ONCOLOGY.drop(columns=['authors','corresponding','current_organization_id','raw_affiliation','index'])
ONCOLOGY['AuthorNumber'] = ONCOLOGY.groupby(['id']).cumcount()+1; 

#Separate the Affiliations subfield of Authors into separate records with duplicated pub_id information
ONCOLOGY=ONCOLOGY.explode('affiliations').reset_index()
AFFIL = pd.json_normalize(ONCOLOGY['affiliations'])
# author.affiliations fields:  city	city_id	country	country_code	id	name	raw_affiliation	state	state_code
AFFIL=AFFIL[['id','name','raw_affiliation']] # <----  have to be real names 
ONCOLOGY[['RORG_ID','RORG_Name','raw_affiliation']] =  AFFIL #<----  can rename - but keep order
ONCOLOGY = ONCOLOGY.drop(columns=['affiliations'])

#Clear up dupes taking only the first Raw Affiliation record
ONCOLOGY = ONCOLOGY.drop_duplicates(subset=['id','last_name','researcher_id',
                                            'first_name','RORG_ID','RORG_Name'], keep="first", inplace=False)

#Explode Open Access onto multiple lines and delete records with OA_All to compare Gold, Green, Closed, etc. without double count risk
ONCOLOGY=ONCOLOGY.explode('open_access').reset_index()
ONCOLOGY = ONCOLOGY[ONCOLOGY.open_access != "oa_all"]

#Identify First and Last Authors (Be sure you haven't sorted or filtered Dataframe to this point!)
ONCOLOGY['AuthorCategory'] = np.where(
     ONCOLOGY['AuthorNumber']==1, 
    'FirstAuthor', 
     np.where(
        (ONCOLOGY['authors_count']-ONCOLOGY['AuthorNumber'])==0,"LastAuthor",""
     )
)

#Filter to Publications/Authors where My or My Peer organization is the first or last author.
print(len(ONCOLOGY))
ONCOLOGY = ONCOLOGY[(ONCOLOGY["AuthorCategory"].isin(['LastAuthor','FirstAuthor']))]
print(len(ONCOLOGY))
ONCOLOGY = ONCOLOGY[(ONCOLOGY["RORG_ID"].isin(["grid.21925.3d","grid.147455.6","grid.25879.31","grid.29857.31"]))]
print(len(ONCOLOGY))

ONCOLOGY.head(9)

"""##Connect Cited and Citing Publications (ListChunk w/ SQL)"""

#   STEP 1:
##Find the id's of interesting publications:
CRBPubs = dsl.query_iterative(f"""search publications  
                                  in title_abstract_only for "coral" or 
                                  in title_abstract_only for "rabbitfish" or 
                                  in title_abstract_only for "barnacle"
                                  where year = 1980
                                  return publications[id+year+times_cited]""", verbose=True).as_dataframe()  
CRBPubs.head(4)
#Drop nulls to avoid errors:
RPs = CRBPubs['id'].dropna().tolist()
print(RPs)
print(len(RPs)," Records in RPs")

CitedPubsList = ['pub.1015378969','pub.1063548752','pub.1011332010']
#   STEP 2:
#Pull information about publications that cited one or more of the publications identified above:
ChunkNumber = 1
#
# Find publications that cited any of the publications in the list above
q = """search publications
              where reference_ids in {} and year in [1986:2023]
              return publications[id+year+reference_ids]"""
#
results = []
for chunk in (list(chunks_of(list(CitedPubsList), 50))): 
    print("Working on Chunk #",(ChunkNumber))
    ChunkNumber = ChunkNumber+1
    data = dsl.query_iterative(q.format(json.dumps(chunk)), verbose=False)
    results += data.publications
    time.sleep(1)
#
# put the data into a dataframe, remove duplicates
CitingPubs = pd.DataFrame().from_dict(results)
print("Publications found: ", len(CitingPubs))
CitingPubs.drop_duplicates(subset='id', inplace=True)
print("Unique publications found: ", len(CitingPubs))
CitingPubs.head(5)

CitingPub=CitingPubs.explode('reference_ids').reset_index()
CitingPub.head(9)


#Join and merge (can also do with Python Syntax)
#Summarize by Cited ID and Citing Publication Year
##!pip install pandasql
##from pandasql import sqldf
##import pandasql as ps


q = """
SELECT ed.id as citedid, ed.times_cited as TotalCitations, ed.year as PublicationYear, 
       ing.id as citingid, ing.year as citation_year
    from CRBPubs ed 
    left outer join CitingPub ing on ing.reference_ids = ed.id
    order by ed.id"""
JOINED = sqldf(q, globals()) 
print (len(JOINED),"Author/Publication/Most Relevant Concept Combinations")
JOINED.head(100)

q = """
SELECT ed.id as citedid, ed.year as PublicationYear, ing.year as citation_year,
       count(distinct(ing.id)) as CitationCount
    from CRBPubs ed 
    left outer join CitingPub ing on ing.reference_ids = ed.id
    group by ed.id, ed.year, ing.year
    order by ed.id"""
JOINEDandGrouped = sqldf(q, globals()) 
print (len(JOINEDandGrouped),"Citations by Year")
JOINEDandGrouped.head(100)

"""##Prepare a Mini Data Warehouse to connect directly in PowerBI"""

##Instead of the "standard" connection information in the first cell in this "Starter Kit", use the following connection code:
##You will need to have a Python Environment set up on your machine and point PowerBI to that Environment.
##(If you've only used COLAB, this will be an extra step and it can be a little tricky)
##https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-python-scripts

##The below will result in a "mini warehouse" of tables that can be refreshed with one-click in PowerBI.
##Design the calls with the keys and detail you need to make the joins or lookups in PowerBI or Pandas as you see fit
##This method is likely preferable to a giant flat exploded table with lots of redundant data


##################Sample Code to embed Starts Here

import requests
import datetime
import dimcli
from dimcli.utils import *
import json
import sys
import pandas as pd
import numpy as np
                  #############Enter your key below
dimcli.login(key="YOUR KEY HERE",
             endpoint="https://app.dimensions.ai")
dimcli
dsl = dimcli.Dsl() 

Authors = dsl.query_iterative("""search publications 
                    where year in [2019:2022] and concepts = "octopus" 
                    return publications[id+authors]""", verbose=True).as_dataframe_authors_affiliations()  ##researcher_id, pub_id, current_organization_ID

Pubs = dsl.query_iterative("""search publications 
                    where year in [2019:2022] and concepts = "octopus" 
                    return publications[id+title+year+times_cited+open_access+publisher+journal]""", verbose=True).as_dataframe()  

RORGS = dsl.query_iterative("""search publications 
                    where year in [2019:2022] and concepts = "octopus" 
                    return publications[id+unnest(research_orgs)]""", verbose=True).as_dataframe()  ##research_orgs.id, research_orgs.name, research_orgs.types, research_orgs.county_name

Concepts = dsl.query_iterative("""search publications 
                    where year in [2019:2022] and concepts = "octopus" 
                    return publications[id+concepts_scores]""", verbose=True).as_dataframe_concepts()  ##research_orgs.id, research_orgs.name, research_orgs.types, research_orgs.county_name

SourceTitles = dsl.query("""search publications where concepts = "octopus"  
                    return source_title limit 1000""", verbose=True).as_dataframe()  ##research_orgs.id, research_orgs.name, research_orgs.types, research_orgs.county_name

##################Code to embed ENDS Here