# -*- coding: utf-8 -*-
"""ResearcherFinder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w1PQD6iYmKh7S5VdvlseAwr4OU3H5CrY

## Do all the python prep stuff and Connect to API
"""

!pip install requests
!pip install dimcli -U
!pip install pandasql
import requests
import datetime
import dimcli
from dimcli.utils import *
import json
import sys
import pandas as pd
import numpy as np
from pandasql import sqldf
import pandasql as ps
#Add Dictionary for fuzzywuzzy which will help score our matches
!pip install fuzzywuzzy[speedup]
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
#https://pypi.org/project/fuzzywuzzy/
print("==\nCHANGELOG\nThis notebook was last run on %s\n==" % datetime.date.today().strftime('%b %d, %Y'))

##there are more secure ways to do this than typing one's key into the code, but we'll come back to that later
dimcli.login(key="YOUR API KEY HERE",
             endpoint="https://app.dimensions.ai")
dimcli
dsl = dimcli.Dsl()

"""##Function to Prepare Dimensions data for merge with MyResearchers"""

# Function to Prep Dimensions Data for merge
#we won't use this until later, but we're defining it here at the beginning
#This is going to work on all calls that use the following return statement:

#return researchers[id+last_name+first_name+first_publication_year+
#                   last_publication_year+total_grants+nih_ppid+
#                   total_publications+orcid_id+unnest(research_orgs)]

#TO call this function later:
#DesiredDataFrameName = DimPrepForMergeWithMyResearchers(Name of dataframe to process)


def DimPrepForMergeWithMyResearchers(DataFrameToProcess):
  DataFrameToProcess =DataFrameToProcess.explode('orcid_id').explode('nih_ppid').reset_index()
  
  DataFrameToProcess = DataFrameToProcess.rename({'last_name':'DIMLastName', 'first_name':'DIMFirstName', 
                                                  'research_orgs.name':'DimInstitution', 
                                                  'research_orgs.id': 'DimGridID'}, axis=1)
  DataFrameToProcess['DimNIH_PPID'] = DataFrameToProcess['nih_ppid'].astype('Int64')
  DataFrameToProcess = DataFrameToProcess[['id','DIMLastName', 'DIMFirstName','DimGridID',
                                           'DimInstitution','DimNIH_PPID','first_publication_year',
                                           'last_publication_year', 'total_grants', 'total_publications',
                                           'orcid_id','Round']]
  DataFrameToProcess['DIMFirstSubst'] = DataFrameToProcess['DIMFirstName'].str.slice(0, 4)
  DataFrameToProcess['DIMLastSubst'] = DataFrameToProcess['DIMLastName'].str.slice(0, 6)
  DataFrameToProcess['DIMFirstSubst'] = DataFrameToProcess['DIMFirstSubst'].str.lower()
  DataFrameToProcess['DIMLastSubst'] = DataFrameToProcess['DIMLastSubst'].str.lower()

  DataFrameToProcess['DimLastFirst'] = DataFrameToProcess['DIMLastName']+', '+DataFrameToProcess['DIMFirstName']
  DataFrameToProcess['DimLastFirst'] = DataFrameToProcess['DimLastFirst'].str.lower()
  DataFrameToProcess['DimLastFirstInst'] = DataFrameToProcess['DimInstitution']+' '+DataFrameToProcess['DimLastFirst']
  DataFrameToProcess['DimLastFirstInst'] = DataFrameToProcess['DimInstitution'].str.lower()
  print(len(DataFrameToProcess))
  DataFrameToProcess.drop_duplicates(subset=['DimLastFirstInst','id','orcid_id','DimNIH_PPID','Round'], inplace=True)
  DataFrameToProcess.sort_values(by = ['DIMLastName','DIMFirstName'], ascending=[True,True])
  print(len(DataFrameToProcess))
  return DataFrameToProcess

"""##Upload csv files of researchers and prepare it for merging"""

#Upload Interesting Names (Watch for prompt when running this cell):
from google.colab import files
uploaded = files.upload()
#You will be prompted to browse to a file.  Put the file name below:
import io
MyResearchers = pd.read_csv(io.BytesIO(uploaded[r'MyResearchers.csv']),encoding='latin1')
MyResearchers.head(3)
#The Example .csv file used in this code has the following fields and fieldNames:
#OrganizationName, PI Name, PILastName, PIFirstName, PIFirstFirst, PI Person ID, DEPT NAME, NIH Dept Combining Name, DEPT TYPE
#For a .csv file with differing fields, you will need to make further edits to this cell and/or remaining code.

# Prep MyResearchers for Merge
cols = MyResearchers.select_dtypes(['object']).columns
MyResearchers[cols] = MyResearchers[cols].apply(lambda x: x.str.strip())

#Create Fields to to use as Merge Keys to join with Dimensions Data:
MyResearchers['MR_FirstSubst'] = MyResearchers['PIFirstName'].str.slice(0, 4)
MyResearchers['MR_LastSubst'] = MyResearchers['PILastName'].str.slice(0, 6)
MyResearchers['MR_FirstSubst'] = MyResearchers['MR_FirstSubst'].str.lower()
MyResearchers['MR_LastSubst'] = MyResearchers['MR_LastSubst'].str.lower()

#Create fields to compare/score MyResearchers data to Dimensions Data
MyResearchers['MR_LastFirst'] = MyResearchers['PILastName']+', '+MyResearchers['PIFirstName']
MyResearchers['MR_NameInst'] = MyResearchers['OrganizationName']+' '+MyResearchers['MR_LastFirst']

MyResearchers['MR_LastFirst'] = MyResearchers['MR_LastFirst'].str.lower()
MyResearchers['MR_NameInst'] = MyResearchers['MR_NameInst'].str.lower()

#Rename some fields for Clarity
MyResearchers = MyResearchers.rename({'DEPT NAME':'MR_Concept', 'PI Person ID':'MR_NIH_PPID'}, axis=1)

MyResearchers=MyResearchers.astype(str)
MyResearchers['MR_NIH_PPID']=MyResearchers['MR_NIH_PPID'].astype(int)
print(len(MyResearchers))
MyResearchers.head(3)

"""##Make Various API calls to find possible matches for the names on our list

###Round 1 - Start with your best identifier (ie ORCID, NIH PPID)
"""

#Since we have a PPID in our csv file, we will create a list of those IDs and then search for them in the 
#Researchers datasource in chunks
#Round 1 - PPID

PPIDs = MyResearchers['MR_NIH_PPID']
#print(PPIDs)

ChunkNumber = 1
ChunkSize = 100  #<--  If you get an error, reduce this number.  Max is 500; 200 is a great starting point.
TotalChunks = round(0.5+(len(PPIDs)/ChunkSize))
q = """search researchers 
       where nih_ppid in {}
       and obsolete = 0
       return researchers[id+last_name+first_name+first_publication_year+last_publication_year+
       total_grants+nih_ppid+total_publications+orcid_id+unnest(research_orgs)]"""
results = []
for chunk in (list(chunks_of(list(PPIDs), ChunkSize))): 
    print("Working on Chunk #",(ChunkNumber)," of ",TotalChunks)
    ChunkNumber = ChunkNumber+1
    data = dsl.query_iterative(q.format(json.dumps(chunk)), verbose=False)
#    data = dsl.query_iterative(q.format(chunk), verbose=False)
    results += data.researchers
    time.sleep(1)
Round1_Call = pd.DataFrame().from_dict(results)
#Round1.drop_duplicates(subset=['id','research_orgs.id'], inplace=True)
Round1_Beta = Round1_Call
Round1_Beta['Round'] = '1st Round - PPID'
print("researcher/institution combinations: ",len(Round1_Beta))
count = Round1_Beta.id.unique().size
print("Unique researcher IDs found in Round1: "+ str(count))

Round1_Beta = DimPrepForMergeWithMyResearchers(Round1_Beta)
Round1_Beta.head(5)

Round1 = pd.merge(
    left=MyResearchers,
    right=Round1_Beta,
    left_on=['MR_NIH_PPID'],
    right_on=['DimNIH_PPID'],
    how='left'
)
print(len(Round1)," Possible Matches")
Round1.head(4)

#How many and Who do we still need to find? Is it worth going to Round 2?
print(len(Round1[Round1['id'].isnull()]), "Remaining unmatched Record(s)")
(Round1[Round1['id'].isnull()]).head(len(Round1[Round1['id'].isnull()]))

"""##Round 2 - Call all researchers ever affiliated and look for name substring matches for MyResearchers where no match was found in Round 1"""

#Round 2 - Find any researchers ever affiliated with MyInstitution: (includes literature, biology, business, engineering, medical, etc. researchers)

#Create a mini MyResearchers with only those we haven't found a PPID Match for
MyResearchers_R2 = Round1[Round1['id'].isnull()] 
MyResearchers_R2 = MyResearchers_R2[MyResearchers_R2.columns[range(13)]]
print(len(MyResearchers_R2), "Researcher(s) still needing a match")
MyResearchers_R2.head(2)

Round2_Call = dsl.query_iterative(f"""search researchers 
                    where research_orgs in  ["grid.10698.36","grid.410711.2"] 
                    and last_publication_year in [2010:2022] 
                    and obsolete = 0
                    return researchers[id+last_name+first_name+first_publication_year+last_publication_year+total_grants+nih_ppid+
                    total_publications+orcid_id+unnest(research_orgs)]""", verbose=True, force = True).as_dataframe() 
Round2_Call['Round'] = '2nd Round - EverAffiliated'
#^^The call above takes a little time so set the Dataframe name so you can manipulate it without replacing it.
Round2_Call.head(3)
Round2_Call.dtypes

#Call the function to prep for merge
Round2_Beta = DimPrepForMergeWithMyResearchers(Round2_Call)
Round2_Beta.head(5)

Round2 = Round2_Beta

count = Round2.id.unique().size
print("Unique researcher IDs found in Round2 : "+ str(count))
Round2_Beta.dtypes
Round2_Beta.head(3)

Round2 = pd.merge(
    left=MyResearchers_R2,
    right=Round2_Beta,
    left_on=['MR_FirstSubst','MR_LastSubst'],
    right_on=['DIMFirstSubst','DIMLastSubst'],
    how='left'
)

#How many and Who do we still need to find? Is it worth going to Round 2?
print(len(Round2[Round2['id'].isnull()]), "Remaining unmatched Record(s)")
Round2.head(9)

#Round 3 - Leverage Dimensions API Keyword search and speciality/research area


MyResearchers_R3 = Round2[Round2['id'].isnull()] 
MyResearchers_R3 = MyResearchers_R3[MyResearchers_R3.columns[range(13)]]

#Since we have a much shorter list of people now, we can use a more time intensive call that searchers for their name
#in the Authors field and returns information about all researchers on the publications.
#We can concatenate our fields together into DSL Code, then use For Loops to run the different lines
MyResearchers_R3['NameConcept'] = "\""+MyResearchers_R3['PIFirstFirst']+" "+MyResearchers_R3['PILastName']+"\" where (concepts = \""+MyResearchers_R3['MR_Concept']+"\"" +" or category_for.name = \"42 Health Sciences\") and"
MyResearchers_R3['NameConcept'].astype(str)
#By putting our new field into a list, we can loop through our list running one call per person and concatenating the results:
CodeList = MyResearchers_R3['NameConcept'].tolist()

LoopNumber = 0
Round3_Call=pd.DataFrame()
#Two sample options below -- be creative and see what works.
############## Option Example #1 Name and Specialty (Or code snippet of your design) ###################
#for a in CodeList:
#  PubChunk =  %dsldf search publications in authors for {a} research_orgs != "grid.42505.36" return researchers[id] limit 1000
##################################################################

############## Option Example #2 Name Only ###################
NameList = MyResearchers_R3['PIFirstFirst']+" "+MyResearchers_R3['PILastName'].tolist()
for a in NameList:
  PubChunk =  %dsldf search publications in authors for "{a}" where research_orgs != "grid.42505.36" return researchers[id] limit 1000
##############################################################

  Round3_Call = pd.concat([Round3_Call, PubChunk])
  LoopNumber = LoopNumber+1
  print(a, "LoopNumber: ",LoopNumber)
  print("Running total records in Round3_Call:  ",len(Round3_Call))
print(len(Round3_Call))
Round3_Call.head(9)
#Note that Option #1 filtered OUT Joseph, but Idea #2 found him (he's ur.0671652444.03 affiliated with U Wisc)

#Now we have a list of all the researchers that were co-authors on papers authored by people with our list of names or similar
#We will use the ListChunk method to pull the researcher details dataframe to match Round1 and Round2:
IDList = Round3_Call['id'].dropna().tolist()
IDList = list(dict.fromkeys(IDList))

#Pull Researcher Information to join with larger set:
ChunkNumber = 1
ChunkSize = 200
q = """search researchers where id in {} 
       return researchers
       [id+last_name+first_name+first_publication_year+last_publication_year+total_grants+nih_ppid+total_publications+orcid_id+unnest(research_orgs)]"""
results = []
for chunk in (list(chunks_of(list(IDList), ChunkSize))): 
    print("Working on Chunk# ",(ChunkNumber))
    ChunkNumber = ChunkNumber+1
    data = dsl.query_iterative(q.format(json.dumps(chunk)), verbose=True)
    results += data.researchers
    time.sleep(1)
RCHDetail = pd.DataFrame().from_dict(results)
RCHDetail['Round'] = '3rd Round - Search for name in Authors'
RCHDetail.head(3)


#Round3 is now "fresh" out of Dimensions and not ready to be merged.  Call our "prep" function:
Round3_beta = DimPrepForMergeWithMyResearchers(RCHDetail)

#Drop the columns we made for searching
MyResearchers_R3 = MyResearchers_R3[MyResearchers_R3.columns[range(13)]]

# Merge and calculate Ratios -- CASE MATTERS!
Round3 = pd.merge(
    left=MyResearchers_R3,
    right=Round3_beta,
    left_on=['MR_FirstSubst','MR_LastSubst'],
    right_on=['DIMFirstSubst','DIMLastSubst']
    #how='left'
)
Round3.head(100)

#Combine all Rounds to prep for Fuzzy Ratio Calculation:
SuperSet = pd.concat([Round1, Round2, Round3], ignore_index=True)
SuperSet = SuperSet.drop_duplicates(keep='last')

#Calculate FuzzRatios
SuperSet['NameRatio'] = SuperSet.dropna(subset=['MR_LastFirst', 'DimLastFirst']).apply(lambda x: fuzz.ratio(x.MR_LastFirst, x.DimLastFirst), axis=1)
SuperSet['NameInstRatio'] = SuperSet.dropna(subset=['MR_NameInst','DimLastFirstInst']).apply(lambda x: fuzz.ratio(x.MR_NameInst, x.DimLastFirstInst), axis=1)
SuperSet = SuperSet.sort_values(['MR_LastFirst','NameInstRatio','NameRatio'],ascending=[True,False,False])
SuperSet['RIDLink'] =  'https://app.dimensions.ai/discover/publication?search_mode=content&and_facet_researcher='+SuperSet['id']

SuperSet.head(5)

"""###Export Final SuperSet to .csv for human review"""

#Export to Excel for line by line analysis

#Drop ugly columns used only for code:
DF_For_Export = SuperSet.drop(columns=['MR_FirstSubst', 'MR_LastSubst', 'MR_LastFirst', 'MR_NameInst', 'MR_Concept', 'MR_NIH_PPID', 'DIMFirstSubst', 'DIMLastSubst', 'DimLastFirstInst', 'DimLastFirst'])

##Additionally here you can drop records under a certain score or create other logic to futher narrow the list -- 
## ie dropping other records when there is a PPID and institution match 
## or taking only the record with the top score per person.

from google.colab import drive
drive.mount('/content/drive')
path = '/content/drive/My Drive/UNC-CH/SuperSet.csv'
with open(path, 'w', encoding = 'utf-8-sig') as f:
  DF_For_Export.to_csv(f)